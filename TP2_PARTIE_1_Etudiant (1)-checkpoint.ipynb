{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2. PARTIE 1. scikit-learn + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module de Machine Learning en Python : scitkit-learn (sklearn)\n",
    "http://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan :\n",
    "\n",
    "   [- Iris dataset](#1)\n",
    "   \n",
    "   [- Naive Bayes](#2)\n",
    "   \n",
    "   [- Mon Naive Bayes](#3)\n",
    "   \n",
    "   [- Tests](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher le dataset **iris** dans le module sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Analyser les r√©sultats des commandes suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'],\n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(BaseNB)\n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via `partial_fit` method.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like, shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_prior_ : array, shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  class_count_ : array, shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  theta_ : array, shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  sigma_ : array, shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB(priors=None)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB(priors=None)\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      BaseNB\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, priors=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like, shape (n_classes,), optional (default=None)\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module sklearn.naive_bayes:\n",
      "\n",
      "fit(X, y, sample_weight=None) method of sklearn.naive_bayes.GaussianNB instance\n",
      "    Fit Gaussian Naive Bayes according to X, y\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vectors, where n_samples is the number of samples\n",
      "        and n_features is the number of features.\n",
      "    \n",
      "    y : array-like, shape (n_samples,)\n",
      "        Target values.\n",
      "    \n",
      "    sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      "        Weights applied to individual samples (1. for unweighted).\n",
      "    \n",
      "        .. versionadded:: 0.17\n",
      "           Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "        Returns self.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "help(gnb.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On donne ici un exemple d'utilisation du Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (iris.data.shape[0],(iris.target != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) On s√©pare en 2 parties √©gales le dataset avec tirage al√©atoire sans remise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 68,   1,  98,  20,   8, 111, 148,  15,  41,  21,  49,  29,  47,\n",
       "       108, 119, 147, 101,  37, 109,  84,  39,  79,  53,  27,  35,  23,\n",
       "        82,   7, 144,  28, 132,  45,  30,  48, 127,  92,  63,  58, 100,\n",
       "        19,  64, 137,  94, 123, 118,  25,  33,  11,  96, 110, 120, 129,\n",
       "        38,  16,  10,  55, 121, 138, 115,  32,  89, 128, 126,   3,  76,\n",
       "        26,  78,  42,  87,  17,  43, 112,  24, 130,  54])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.random.choice(range(150), 75, replace=False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   5,   6,   9,  12,  13,  14,  18,  22,  31,  34,\n",
       "        36,  40,  44,  46,  50,  51,  52,  56,  57,  59,  60,  61,  62,\n",
       "        65,  66,  67,  69,  70,  71,  72,  73,  74,  75,  77,  80,  81,\n",
       "        83,  85,  86,  88,  90,  91,  93,  95,  97,  99, 102, 103, 104,\n",
       "       105, 106, 107, 113, 114, 116, 117, 122, 124, 125, 131, 133, 134,\n",
       "       135, 136, 139, 140, 141, 142, 143, 145, 146, 149])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.delete(range(150), train)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on regarde le r√©sutat sur la pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "100*(iris.target[test,] != y_pred).sum()/len(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Construire une fonction NB de param√®tre A, B et nb qui r√©p√®te nb tirages al√©atoires avec i donn√©es pour la partie apprentissage et 150-i pour le test avec i allant de A √† B. La fonction renvoie une liste de taille B-A+1 avec le pourcentage de pr√©diction exacte sur les donn√©es de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,  30, 120, 107,  70])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(150), 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71.396396396396383,\n",
       " 25.170068027210885,\n",
       " 42.694063926940636,\n",
       " 54.252873563218394,\n",
       " 43.75,\n",
       " 29.836829836829835,\n",
       " 26.291079812206572,\n",
       " 43.026004728132392,\n",
       " 19.761904761904763,\n",
       " 27.098321342925658,\n",
       " 9.6618357487922708,\n",
       " 17.761557177615572,\n",
       " 24.509803921568629,\n",
       " 24.444444444444446,\n",
       " 14.179104477611938,\n",
       " 18.796992481203009,\n",
       " 13.13131313131313,\n",
       " 20.101781170483459,\n",
       " 5.1282051282051286,\n",
       " 7.4935400516795871,\n",
       " 4.947916666666667,\n",
       " 7.0866141732283454,\n",
       " 4.2328042328042335,\n",
       " 6.1333333333333329,\n",
       " 5.645161290322581,\n",
       " 5.4200542005420056,\n",
       " 6.0109289617486334,\n",
       " 7.4380165289256199,\n",
       " 8.8888888888888893]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NB(A,B,nb):\n",
    "    res = []\n",
    "    for i in range(A,B+1):\n",
    "        temp = 0\n",
    "        for j in range(nb):\n",
    "            train = np.random.choice(range(150), i, replace=False)\n",
    "            test = np.delete(range(150), train)\n",
    "            y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "            temp = temp + 100*(iris.target[test,] != y_pred).sum()/len(test)\n",
    "        res = res + [temp/nb]\n",
    "    return res\n",
    "NB(2,30,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Tracer sur un graphique le vecteur NB(A,B,10) avec A = 2 et B = 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEyCAYAAADeAVWKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdfc29e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = 2\n",
    "B = 149\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(A,B,B-A+1), NB(A,B,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32046798,  0.40386107,  0.1373366 ,  0.10765042]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=iris.data[train,] \n",
    "train_feature=iris.data[train,]  \n",
    "data_test=iris.data[test,] \n",
    "test_feature=iris.data[test,]\n",
    "target_train=iris.target[train,]\n",
    "id_y0=np.where(target_train==0)\n",
    "data_train[id_y0,].shape\n",
    "pi_hat=data_train[id_y0,].shape[1]/data_train.shape[0]\n",
    "np.mean(data_train[id_y0,],axis=1)\n",
    "\n",
    "np.std(data_train[id_y0,],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta(x,mu_hat,sigma_hat,pi_hat):\n",
    "    \"\"\"\n",
    "    fonction discriminante Naive Bayes\n",
    "    x : valeur √† laquelle sera √©valu√©e delta, np.array size d\n",
    "    mu_hat : moyenne des features, np.array size d\n",
    "    sigma_hat : std des features, np.array size d\n",
    "    pi_hat : proportion parmi le train, np.float\n",
    "    \"\"\"\n",
    "    d=x.shape[0]\n",
    "    log_density_features = -d*np.log(np.sqrt(2*np.pi))-np.sum(np.log(sigma_hat)) \\\n",
    "    -np.sum((x-mu_hat)**2/(2*sigma_hat**2))\n",
    "    log_proba=np.log(pi_hat)\n",
    "    return log_density_features+log_proba\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mon Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Si Bayes est si naif que √ßa, pourquoi pas le construire nous-m√™me ! En s'aidant du cours, construire la fonction MonNaiveBayes sur le mod√®le suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonNaiveBayes(train_featues,train_label, test_features):\n",
    "    target_values=np.unique(target_train)\n",
    "    n_train,d_train=data_train.shape\n",
    "    n_test,d_test=data_test.shape\n",
    "    mu_hat=np.zeros((target_values.shape[0],d))\n",
    "    sigma_hat=np.zeros((target_values.shape[0],d))\n",
    "    pi_hat=np.zeros((target_values.shape[0],))  \n",
    "    k=0\n",
    "    for y in target_values:\n",
    "        id_y=np.where(target_train==y)\n",
    "        mu_hat[k,:]=np.mean(data_train[id_y,],axis=1)\n",
    "        sigma_hat[k,:]=np.std(data_train[id_y,],axis=1)\n",
    "        pi_hat[k]=data_train[id_y,].shape[1]/data_train.shape[0]\n",
    "        k+=1\n",
    "    delta_hat_train=np.zeros((n_train,target_values.shape[0]))\n",
    "    delta_hat_test=np.zeros((n_test,target_values.shape[0]))\n",
    "    j=0\n",
    "    for y in target_values:\n",
    "        delta_hat_test[:,j]=delta(test_feature,mu_hat[j],sigma_hat[j],pi_hat[j])\n",
    "        delta_hat_train[:,j]=delta(train_feature,mu_hat[j],sigma_hat[j],pi_hat[j])\n",
    "        j+=1\n",
    "    predict_test = np.argmax(delta_hat_test,axis=1)\n",
    "    predict_train = np.argmax(delta_hat_train,axis=1)\n",
    "    res={\"predict_test\" : predict_test, \"predict_train\" : predict_train}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predict_test': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2], dtype=int64), 'predict_train': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "result=MonNaiveBayes(data_train,target_train, iris.data[test,])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Comparer le r√©sultat au r√©sultat de la fonction de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict_test': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2], dtype=int64),\n",
       " 'predict_train': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2], dtype=int64)}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "iris2 = copy.deepcopy(iris)\n",
    "\n",
    "iris2.data = iris2.data[3:30,]\n",
    "iris2.target = iris2.target[3:30,]\n",
    "MonNaiveBayes(iris, iris2,iris.data[test,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
